{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b02d04af-88d2-4f56-a27a-712cbec154fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d260a4-c415-44c4-9656-85e2c846bc21",
   "metadata": {},
   "source": [
    "## Extracting weather data from persistent folder in landing zone\n",
    "\n",
    "### Weather Data Summary\n",
    "- `SOUID` refers to weather station ID and has 2 unique values\n",
    "    - '116436' and '903772' both refer to weather stations near Heathrow Airport\n",
    "- `DATE` refers to date of wather measurement\n",
    "    - measurements range from '1979-01-01' to '2020-12-31'\n",
    "- `CC` cloud cover measurement in **oktas**\n",
    "- `SS` sunshine measurement in **0.1 Hours**\n",
    "- `QQ` global radiation measurement in **W/m2**\n",
    "- `TX` maximum temperature measurement in **0.1 °C**\n",
    "- `TG` mean temperature measurement in **0.1 °C**\n",
    "- `TN` minimum temperature measurement in **0.1 °C**\n",
    "- `RR` precipitation measurement scaled in **0.1 mm**\n",
    "- `PP` pressure measurement in **0.1 hPa**\n",
    "- `SD` snow depth measurement in **1 cm**\n",
    "- `Q_` refers to quality of weather measurement\n",
    "    - from trusted (0), to dubious (1), and incorrect (9)\n",
    "\n",
    "**Missing Values:**\n",
    "- Measurements that are missing are marked as -9999 and have `Q_` of 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec4ee584-933a-4ff1-8b92-1c422e15b0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing London weather data\n",
    "cloud_cover_df = pd.read_csv('persistent/London_weather/cloud_cover/CC_STAID001860.txt', sep=\",\")\n",
    "sunshine_df = pd.read_csv('persistent/london_weather/sunshine/SS_STAID001860.txt', sep=\",\")\n",
    "global_radiation_df = pd.read_csv('persistent/london_weather/global_radiation/QQ_STAID001860.txt', sep=\",\")\n",
    "max_temp_df = pd.read_csv('persistent/london_weather/max_temperature/TX_STAID001860.txt', sep=\",\")\n",
    "mean_temp_df = pd.read_csv('persistent/london_weather/mean_temperature/TG_STAID001860.txt', sep=\",\")\n",
    "min_temp_df = pd.read_csv('persistent/london_weather/min_temperature/TN_STAID001860.txt', sep=\",\")\n",
    "precipitation_df = pd.read_csv('persistent/london_weather/precipitation_amount/RR_STAID001860.txt', sep=\",\")\n",
    "pressure_df = pd.read_csv('persistent/london_weather/sea_level_pressure/PP_STAID001860.txt', sep=\",\")\n",
    "snow_depth_df = pd.read_csv('persistent/london_weather/snow_depth/SD_STAID001860.txt', sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53862c5d-d4b2-4045-987a-a9c5afb850b2",
   "metadata": {},
   "source": [
    "### Merging all weather measurement dataframes\n",
    "This will be done to simplify exporting to PostgreSQL as well as simplifying data flow moving forward. The process is simple because all dataframes have same range of date values as well as valid values for each date. This ensures resulting dataframe will be complete\n",
    "\n",
    "**Issues to fix:**\n",
    "- Column names are formatted improperly with whitespaces\n",
    "    - Discrepancies in names are the same for all dataframes\n",
    "    - This will be fixed as a preliminary step for all dataframes\n",
    "- Missing values are represented by -9999\n",
    "    - They will be assigned NaN\n",
    "- Measurements have different units and scalings\n",
    "    - These will be treated in the trusted zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1129c49-1477-4ff5-bb9d-7cfb9f0f854a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatting column names correctly\n",
    "cloud_cover_df = cloud_cover_df.rename({' SOUID': 'SOUID', '    DATE': 'DATE', '   CC': 'CC', ' Q_CC': 'Q_CC'}, axis=1)  # cloud cover\n",
    "sunshine_df = sunshine_df.rename({' SOUID': 'SOUID', '    DATE': 'DATE', '   SS': 'SS', ' Q_SS': 'Q_SS'}, axis=1)  # sunshine\n",
    "global_radiation_df = global_radiation_df.rename({' SOUID': 'SOUID', '    DATE': 'DATE', '   QQ': 'QQ', ' Q_QQ': 'Q_QQ'}, axis=1)  # global radiation\n",
    "max_temp_df = max_temp_df.rename({' SOUID': 'SOUID', '    DATE': 'DATE', '   TX': 'TX', ' Q_TX': 'Q_TX'}, axis=1)  # max temp\n",
    "mean_temp_df = mean_temp_df.rename({' SOUID': 'SOUID', '    DATE': 'DATE', '   TG': 'TG', ' Q_TG': 'Q_TG'}, axis=1)  # mean temp\n",
    "min_temp_df = min_temp_df.rename({' SOUID': 'SOUID', '    DATE': 'DATE', '   TN': 'TN', ' Q_TN': 'Q_TN'}, axis=1)  # min temp\n",
    "precipitation_df = precipitation_df.rename({' SOUID': 'SOUID', '    DATE': 'DATE', '   RR': 'RR', ' Q_RR': 'Q_RR'}, axis=1)  # precipitation\n",
    "pressure_df = pressure_df.rename({' SOUID': 'SOUID', '    DATE': 'DATE', '   PP': 'PP', ' Q_PP': 'Q_PP'}, axis=1)  # pressure\n",
    "snow_depth_df = snow_depth_df.rename({' SOUID': 'SOUID', '    DATE': 'DATE', '   SD': 'SD', ' Q_SD': 'Q_SD'}, axis=1)  # snow depth\n",
    "\n",
    "# dropping SOUID from all dataframes\n",
    "cloud_cover_df = cloud_cover_df.drop(['SOUID'], axis=1)\n",
    "sunshine_df = sunshine_df.drop(['SOUID'], axis=1)\n",
    "global_radiation_df = global_radiation_df.drop(['SOUID'], axis=1)\n",
    "max_temp_df = max_temp_df.drop(['SOUID'], axis=1)\n",
    "mean_temp_df = mean_temp_df.drop(['SOUID'], axis=1)\n",
    "min_temp_df = min_temp_df.drop(['SOUID'], axis=1)\n",
    "precipitation_df = precipitation_df.drop(['SOUID'], axis=1)\n",
    "pressure_df = pressure_df.drop(['SOUID'], axis=1)\n",
    "snow_depth_df = snow_depth_df.drop(['SOUID'], axis=1)\n",
    "\n",
    "# merging into single dataframe\n",
    "weather_df = cloud_cover_df.merge(sunshine_df, on='DATE')\n",
    "weather_df = weather_df.merge(global_radiation_df, on='DATE')\n",
    "weather_df = weather_df.merge(max_temp_df, on='DATE')\n",
    "weather_df = weather_df.merge(mean_temp_df, on='DATE')\n",
    "weather_df = weather_df.merge(min_temp_df, on='DATE')\n",
    "weather_df = weather_df.merge(precipitation_df, on='DATE')\n",
    "weather_df = weather_df.merge(pressure_df, on='DATE')\n",
    "weather_df = weather_df.merge(snow_depth_df, on='DATE')\n",
    "\n",
    "# replacing -9999 values with NaN\n",
    "weather_df['CC'] = weather_df['CC'].map(lambda x: np.nan if x in [-9999] else x)\n",
    "weather_df['SS'] = weather_df['SS'].map(lambda x: np.nan if x in [-9999] else x)\n",
    "weather_df['QQ'] = weather_df['QQ'].map(lambda x: np.nan if x in [-9999] else x)\n",
    "weather_df['TX'] = weather_df['TX'].map(lambda x: np.nan if x in [-9999] else x)\n",
    "weather_df['TG'] = weather_df['TG'].map(lambda x: np.nan if x in [-9999] else x)\n",
    "weather_df['TN'] = weather_df['TN'].map(lambda x: np.nan if x in [-9999] else x)\n",
    "weather_df['RR'] = weather_df['RR'].map(lambda x: np.nan if x in [-9999] else x)\n",
    "weather_df['PP'] = weather_df['PP'].map(lambda x: np.nan if x in [-9999] else x)\n",
    "weather_df['SD'] = weather_df['SD'].map(lambda x: np.nan if x in [-9999] else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56b00054-46f1-4806-b003-ceeb3c60dee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>CC</th>\n",
       "      <th>Q_CC</th>\n",
       "      <th>SS</th>\n",
       "      <th>Q_SS</th>\n",
       "      <th>QQ</th>\n",
       "      <th>Q_QQ</th>\n",
       "      <th>TX</th>\n",
       "      <th>Q_TX</th>\n",
       "      <th>TG</th>\n",
       "      <th>Q_TG</th>\n",
       "      <th>TN</th>\n",
       "      <th>Q_TN</th>\n",
       "      <th>RR</th>\n",
       "      <th>Q_RR</th>\n",
       "      <th>PP</th>\n",
       "      <th>Q_PP</th>\n",
       "      <th>SD</th>\n",
       "      <th>Q_SD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19790101</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-41.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-75.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10190.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19790102</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-75.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10253.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19790103</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-72.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10205.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19790104</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-65.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10084.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19790105</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10225.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15336</th>\n",
       "      <td>20201227</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9800.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15337</th>\n",
       "      <td>20201228</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-13.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9737.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15338</th>\n",
       "      <td>20201229</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9883.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15339</th>\n",
       "      <td>20201230</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10020.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15340</th>\n",
       "      <td>20201231</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-31.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10050.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15341 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           DATE   CC  Q_CC  SS  Q_SS    QQ  Q_QQ    TX  Q_TX    TG  Q_TG  \\\n",
       "0      19790101  2.0     0  70     0  52.0     0  23.0     0 -41.0     0   \n",
       "1      19790102  6.0     0  17     0  27.0     0  16.0     0 -26.0     0   \n",
       "2      19790103  5.0     0   0     0  13.0     0  13.0     0 -28.0     0   \n",
       "3      19790104  8.0     0   0     0  13.0     0  -3.0     0 -26.0     0   \n",
       "4      19790105  6.0     0  20     0  29.0     0  56.0     0  -8.0     0   \n",
       "...         ...  ...   ...  ..   ...   ...   ...   ...   ...   ...   ...   \n",
       "15336  20201227  1.0     0   9     0  32.0     0  75.0     0  75.0     0   \n",
       "15337  20201228  7.0     0  37     0  38.0     0  36.0     0  11.0     0   \n",
       "15338  20201229  7.0     0   0     0  21.0     0  41.0     0  26.0     0   \n",
       "15339  20201230  6.0     0   4     0  22.0     0  56.0     0  27.0     0   \n",
       "15340  20201231  7.0     0  13     0  34.0     0  15.0     0  -8.0     0   \n",
       "\n",
       "         TN  Q_TN    RR  Q_RR       PP  Q_PP   SD  Q_SD  \n",
       "0     -75.0     0   4.0     0  10190.0     0  9.0     0  \n",
       "1     -75.0     0   0.0     0  10253.0     0  8.0     0  \n",
       "2     -72.0     0   0.0     0  10205.0     0  4.0     0  \n",
       "3     -65.0     0   0.0     0  10084.0     0  2.0     0  \n",
       "4     -14.0     0   0.0     0  10225.0     0  1.0     0  \n",
       "...     ...   ...   ...   ...      ...   ...  ...   ...  \n",
       "15336  76.0     1  20.0     0   9800.0     0  NaN     9  \n",
       "15337 -13.0     0   2.0     0   9737.0     0  NaN     9  \n",
       "15338  11.0     0   0.0     0   9883.0     0  NaN     9  \n",
       "15339  -1.0     0   0.0     0  10020.0     0  NaN     9  \n",
       "15340 -31.0     0   0.0     0  10050.0     0  NaN     9  \n",
       "\n",
       "[15341 rows x 19 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccd87ee-bca2-4654-8753-7a02bce623bf",
   "metadata": {},
   "source": [
    "### Exporting Weather Data to Formatted Zone Database in PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f92e4f1f-10af-4816-9d7b-e36160d8ba82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create an engine instance\n",
    "conn_string = 'postgresql://postgres:ETS80321123GOM1!@localhost:5432/formatted_zone'\n",
    "db = create_engine(conn_string)\n",
    "\n",
    "# Connect to PostgreSQL server\n",
    "conn = db.connect()\n",
    "\n",
    "# Load weather data from dataframe into PostgreSQL database table named weather\n",
    "weather_df.to_sql('weather', con=conn, if_exists='replace', index=False)\n",
    "\n",
    "# conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789a8415-1c21-49f5-a749-0b6b8731b74b",
   "metadata": {},
   "source": [
    "#### If we would like to query our new table we could use the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc6ee3d-a027-4a72-9e42-1e6f521064b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to PostgreSQL server\n",
    "conn = psycopg2.connect(conn_string)\n",
    "conn.set_session(autocommit=True) # autocommit\n",
    "\n",
    "# Instantiating cursor\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Your SQL query below...\n",
    "sql1 = '''select DATE, count(*) from weather;'''\n",
    "cur.execute(sql1)\n",
    "for i in cur.fetchall():\n",
    "    print(i)\n",
    "\n",
    "# closing connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e550cd-4f56-4355-8f40-6d12ae123b60",
   "metadata": {},
   "source": [
    "## Extracting energy data from persistent folder in landing zone\n",
    "\n",
    "### Energy Data Summary\n",
    "- `LCLid` refers to household ID and has 5566 unique values\n",
    "- `stdorToU` refers o whether house is in standard sub-category or in the ToT category which keeps track of energy consumption cost\n",
    "    - there are 2 unique values: **std** and **ToU**\n",
    "- `DateTime` refers to date and time of energy consumption measurement\n",
    "    - measurements range from '2011-11-01' to '2014-02-28'\n",
    "- `KWH/hh` refers to energy expenditure measured in KWH per half hour\n",
    "    - This is a continuous variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82156d64-4f1b-460e-bcc6-7380c8246f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing London energy data\n",
    "energy_df = pd.read_csv('persistent/London_energy/London_energy.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202c8477-2d65-44c9-a4ca-330b0edbefc8",
   "metadata": {},
   "source": [
    "### Summarizing Energy Data\n",
    "Raw dataset contains energy consumption per household for every half hour. We will condense this down to daily consumption by summing half hour consumption by household and by date. This will be done in order to reduce the size of the dataset (8Gb) as well as simplifying flow of data moving forward. The dataset also needs to be condensed to daily values in order to be compatible with daily weather data.\n",
    "\n",
    "**Issues to fix:**\n",
    "- `KWH` column has wrongly formatted name with whitespaces\n",
    "- `KWH` values are chr type\n",
    "    - They will be changed to float\n",
    "- `DateTime` column will be split into 'Date' and 'Time'\n",
    "    - `Date` will be kept in order to aggregate data\n",
    "    - `Time` will be dropped in order to properly aggregate data by Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "47dd69d0-72f6-47aa-93b1-fd698ae7b4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming columns\n",
    "energy_df.rename({'KWH/hh (per half hour) ': 'KWH'}, axis=1, inplace=True)\n",
    "\n",
    "# setting chr values in 'KWH' to np.nan\n",
    "energy_df['KWH'] = energy_df['KWH'].map(lambda x: np.nan if x in ['Null', ' '] else x)\n",
    "# setting 'KWH' type to float\n",
    "energy_df['KWH'] = energy_df['KWH'].astype(float)\n",
    "\n",
    "# splitting 'DateTime' into separate 'Date' and 'Time'\n",
    "energy_df['Date'] = pd.to_datetime(energy_df['DateTime']).dt.date\n",
    "energy_df['Time'] = pd.to_datetime(energy_df['DateTime']).dt.time\n",
    "\n",
    "# Dropping 'Time' and 'DateTime' from energy_df\n",
    "energy_df_byday = energy_df.drop(['Time', 'DateTime', 'stdorToU'], axis=1)\n",
    "\n",
    "# Sum of KWH by 'LCLid' and 'Date'\n",
    "sum_energy_df_byday = energy_df_byday.groupby(['LCLid','Date']).sum().reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feec4ceb-ccdc-477c-8c9f-6aaf890d1567",
   "metadata": {},
   "source": [
    "### Exporting Energy Data to Formatted Zone Database in PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda22441-ec1c-4e46-8656-18918c4e0318",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create an engine instance\n",
    "conn_string = 'postgresql://postgres:****************@localhost:5432/formatted_zone'\n",
    "db = create_engine(conn_string)\n",
    "\n",
    "# Connect to PostgreSQL server\n",
    "conn = db.connect()\n",
    "\n",
    "# Load energy data from dataframe into PostgreSQL database table named energy\n",
    "sum_energy_df_byday.to_sql('energy', con=conn, if_exists='replace', index=False)\n",
    "\n",
    "# conn.commit()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
