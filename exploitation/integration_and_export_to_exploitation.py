#!/usr/bin/env python
# coding: utf-8

# In[50]:


import pandas as pd
import numpy as np
from sqlalchemy import create_engine


# ## Importing Weather and Energy Tables from Trusted Zone

# In[51]:


# Create an engine instance
conn_string = 'postgresql://postgres:****************@localhost:5432/trusted_zone'
db = create_engine(conn_string)

# Connect to PostgreSQL server
conn = db.connect()

# Read weather data from PostgreSQL database table and load into weather_df
weather_df = pd.read_sql('select * from "weather"', conn)
pd.set_option('display.expand_frame_repr', False)

# Read energy data from PostgreSQL database table and load into energy_df
energy_df = pd.read_sql('select * from "energy"', conn)
pd.set_option('display.expand_frame_repr', False)

# Print DataFrame Summaries
print('-'*30 + '\n' + 'Weather Data Summary\n' + '-'*30)
print(weather_df.info())
print(weather_df.head())

print('\n'*2 + '-'*30 + '\n' + 'Energy Data Summary\n' + '-'*30)
print(energy_df.info())
print(energy_df.head())

# Close the database connection
conn.close()


# ## Weather Table Preparation for Integration
# **Steps to complete:**
# - Removing 'Q_' values
#     - This is not essential for integration but we no longer have any use for 'Q_' columns moving forward
# - Proper unit scaling of each measurement
#     - This is also not essential but will be done to simplify necessary processes for creating proper visualizations in the Feature Generation Zone
# - Selecting a range of dates for weather measurements that is also available in our energy consumption table (2011-11-23 to 2014-02-28)
#     - This is a necessary step in order to integrate both datasets
# 
# ### Removing `Q_` values from Weather Data
# We can do this at this point in the pipeline because observations with values of 9 (missing) were already treated in the Trusted Zone

# In[52]:


# dropping all 'Q_' columns from dataset
weather_df = weather_df.drop(['Q_CC', 'Q_SS', 'Q_QQ', 'Q_TX', 'Q_TG', 'Q_TN', 'Q_RR', 'Q_PP', ], axis=1)


# ### Unit scaling of measurements
# - `cloud_cover` measurement in **oktas**
#     - will be left as **oktas**
# - `sunshine` measurement in **0.1 Hours**
#     - will be scaled to **1 Hours**
# - `glob_radiation` measurement in **W/m2**
#     - will be left as **W/m2**
# - `max_temp` measurement in **0.1 °C**
#     - will be scaled to **1 °C**
# - `mean_temp` measurement in **0.1 °C**
#     - will be scaled to **1 °C**
# - `min_temp` measurement in **0.1 °C**
#     - will be scaled to **1 °C**
# - `precipitation` measurement scaled in **0.1 mm**
#     - will be scaled to **1 mm**
# - `pressure` measurement in **0.1 hPa**
#     - will be scaled to **1 hPa**
#     
# If all calculations are applied correctly:
# - all measurements for the describe() method will show a reduction for each scaled value (except count) corresponding to a division by 10

# In[53]:


# scaling weather measurement units
weather_df['sunshine'] = weather_df['sunshine'].apply(lambda x: x / 10) # sunshine
weather_df['max_temp'] = weather_df['max_temp'].apply(lambda x: x / 10) # max_temp
weather_df['mean_temp'] = weather_df['mean_temp'].apply(lambda x: x / 10) # mean_temp
weather_df['min_temp'] = weather_df['min_temp'].apply(lambda x: x / 10) # min_temp
weather_df['precipitation'] = weather_df['precipitation'].apply(lambda x: x / 10) # precipitation
weather_df['pressure'] = weather_df['pressure'].apply(lambda x: x / 10) # pressure


# In[54]:


# greater than the start date and smaller than the end date
start_date = '2011-11-23'
end_date = '2014-02-28'
date_mask = (weather_df['date'] >= start_date) & (weather_df['date'] <= end_date)
# mapped weather data
mapped_weather_df = weather_df.loc[date_mask]


# In[55]:


mapped_weather_df


# ## Energy Table Preparation for Integration
# 
# **Steps to complete:**
# - Aggregating the energy consumption of all households per day
#     - This will be done in order to properly map all daily weather measurements without hundreds of thousands of repetitions
#         - Later in the data analysis phase we will attempt to find correlations between temperature and overall energy consumption of a city. We will do this by using our sample of 5567 households as a good predictor of our assumed population.

# In[56]:


# Dropping 'LCLid' from energy_df
energy_df_byday = energy_df.drop(['LCLid'], axis=1)
# Sum of KWH by 'Date'
sum_energy_df_byday = energy_df_byday.groupby(['Date']).sum().reset_index()

# change 'Date' column type to datetime64 and renaming to 'date'
sum_energy_df_byday['Date'] = pd.to_datetime(sum_energy_df_byday['Date'])


# In[57]:


sum_energy_df_byday


# ## Integrating Weather and Energy Datasets on `Date`
# We can do this now that we have performed the necessary processes and have ensured that both tables are compatible (same date range and number of rows).

# In[58]:


# sorting both dataframes by 'Date' thus ensuring compatibility
mapped_weather_df.sort_values(by='date')
sum_energy_df_byday.sort_values(by='Date')

# merging both dataframes
energy_vs_weather_df = pd.merge(sum_energy_df_byday, mapped_weather_df, how='outer', left_index=True, right_index=True)

# drop extra 'date' column
energy_vs_weather_df = energy_vs_weather_df.drop(['date'], axis=1)


# In[61]:


# preview summary for merged dataset
print(energy_vs_weather_df.info())
print('\n')
print(energy_vs_weather_df.describe())


# ## Exporting Results to Exploitation Zone in PostgreSQL

# In[63]:


# Create an engine instance
conn_string = 'postgresql://postgres:****************@localhost:5432/exploitation_zone'
db = create_engine(conn_string)

# Connect to PostgreSQL server
conn = db.connect()

# Load energy vs weather from dataframe into PostgreSQL database table named energy
energy_vs_weather_df.to_sql('energy_vs_weather', con=conn, if_exists='replace', index=False)

# conn.commit()
conn.close()


# In[ ]:




